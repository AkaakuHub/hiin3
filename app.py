import streamlit as st
import numpy as np
import cv2
import mediapipe as mp
from scipy.spatial import procrustes
from streamlit_image_coordinates import streamlit_image_coordinates
from PIL import Image

@st.cache_resource
def initialize_face_landmarker():
    BaseOptions = mp.tasks.BaseOptions
    FaceLandmarker = mp.tasks.vision.FaceLandmarker
    FaceLandmarkerOptions = mp.tasks.vision.FaceLandmarkerOptions
    VisionRunningMode = mp.tasks.vision.RunningMode
    
    options = FaceLandmarkerOptions(
        base_options=BaseOptions(model_asset_path='face_landmarker_v2_with_blendshapes.task'),
        running_mode=VisionRunningMode.IMAGE
    )
    return FaceLandmarker.create_from_options(options)

def extract_landmarks(image, landmarker):
    try:
        # ç”»åƒãŒRGBã®å ´åˆã¯ãã®ã¾ã¾ä½¿ç”¨ã€BGRã®å ´åˆã¯å¤‰æ›
        if len(image.shape) == 3 and image.shape[2] == 3:
            # RGBã¨ä»®å®šã—ã¦å‡¦ç†
            image_rgb = image.copy()
        else:
            st.error(f"ç”»åƒå½¢çŠ¶ãŒäºˆæœŸã—ãªã„å½¢å¼ã§ã™: {image.shape}")
            return None, "ç”»åƒå½¢çŠ¶ã‚¨ãƒ©ãƒ¼"
        
        # ãƒ‡ãƒ¼ã‚¿å‹ã‚’uint8ã«ç¢ºå®Ÿã«å¤‰æ›
        if image_rgb.dtype != np.uint8:
            image_rgb = (image_rgb * 255).astype(np.uint8) if image_rgb.max() <= 1.0 else image_rgb.astype(np.uint8)
        
        # ç”»åƒãŒé€£ç¶šé…åˆ—ã§ã‚ã‚‹ã“ã¨ã‚’ä¿è¨¼
        image_rgb = np.ascontiguousarray(image_rgb)
        
        st.write(f"ãƒ‡ãƒãƒƒã‚°: ç”»åƒå½¢çŠ¶={image_rgb.shape}, ãƒ‡ãƒ¼ã‚¿å‹={image_rgb.dtype}, æœ€å¤§å€¤={image_rgb.max()}, æœ€å°å€¤={image_rgb.min()}")
        
        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image_rgb)
        result = landmarker.detect(mp_image)
        
        if result.face_landmarks and len(result.face_landmarks) > 0:
            landmarks = result.face_landmarks[0]
            points = np.array([[lm.x * image.shape[1], lm.y * image.shape[0], lm.z] for lm in landmarks])
            st.success(f"é¡”ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯æ¤œå‡ºæˆåŠŸ: {len(landmarks)}å€‹ã®ç‚¹")
            return points, None
        else:
            return None, f"é¡”ãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚æ¤œå‡ºã•ã‚ŒãŸé¡”ã®æ•°: {len(result.face_landmarks) if result.face_landmarks else 0}"
    
    except Exception as e:
        st.error(f"ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯æŠ½å‡ºä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return None, f"ã‚¨ãƒ©ãƒ¼: {str(e)}"

def calculate_procrustes_similarity(landmarks1, landmarks2):
    mtx1, mtx2, disparity = procrustes(landmarks1, landmarks2)
    return disparity

def draw_landmarks_on_image(image, landmarks):
    if landmarks is None:
        return image
    
    annotated_image = image.copy()
    height, width = image.shape[:2]
    
    # ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯ã®ã‚µã‚¤ã‚ºã‚’ç”»åƒã‚µã‚¤ã‚ºã«å¿œã˜ã¦èª¿æ•´
    point_size = max(1, min(width, height) // 200)
    
    # MediaPipeã®é¡”ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯æ¥ç¶šæƒ…å ±ï¼ˆä¸»è¦ãªé¡”ã®è¼ªéƒ­ï¼‰
    connections = [
        # é¡”ã®è¼ªéƒ­ (0-16)
        [(i, i+1) for i in range(16)],
        # å·¦çœ‰æ¯› (17-21)
        [(i, i+1) for i in range(17, 21)],
        # å³çœ‰æ¯› (22-26)
        [(i, i+1) for i in range(22, 26)],
        # é¼»ç­‹ (27-30)
        [(i, i+1) for i in range(27, 30)],
        # é¼»ã®ä¸‹éƒ¨ (31-35)
        [(i, i+1) for i in range(31, 35)],
        # å·¦ç›® (36-41)
        [(i, i+1) for i in range(36, 41)] + [(41, 36)],
        # å³ç›® (42-47)
        [(i, i+1) for i in range(42, 47)] + [(47, 42)],
        # å¤–å”‡ (48-59)
        [(i, i+1) for i in range(48, 59)] + [(59, 48)],
        # å†…å”‡ (60-67)
        [(i, i+1) for i in range(60, 67)] + [(67, 60)]
    ]
    
    # ç·šã‚’æç”»ï¼ˆMediaPipeã®å…¨478ç‚¹ã§ã¯è¤‡é›‘ã™ãã‚‹ã®ã§ã€ä¸»è¦ãª68ç‚¹ã®ã¿è¡¨ç¤ºï¼‰
    if len(landmarks) >= 68:
        for connection_group in connections:
            for start_idx, end_idx in connection_group:
                if start_idx < len(landmarks) and end_idx < len(landmarks):
                    start_point = (int(landmarks[start_idx][0]), int(landmarks[start_idx][1]))
                    end_point = (int(landmarks[end_idx][0]), int(landmarks[end_idx][1]))
                    cv2.line(annotated_image, start_point, end_point, (0, 255, 255), 1)
    
    # å…¨ãƒã‚¤ãƒ³ãƒˆã‚’æç”»
    for i, point in enumerate(landmarks):
        x, y = int(point[0]), int(point[1])
        
        # é‡è¦ãªãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯ã¯å¤§ããè¡¨ç¤º
        if i < 68:  # ä¸»è¦ãª68ç‚¹
            if i in [36, 39, 42, 45]:  # ç›®ã®è§’
                cv2.circle(annotated_image, (x, y), point_size + 1, (255, 0, 0), -1)  # é’
            elif i in [48, 54]:  # å£ã®è§’
                cv2.circle(annotated_image, (x, y), point_size + 1, (0, 0, 255), -1)  # èµ¤
            elif i in [30]:  # é¼»ã®å…ˆç«¯
                cv2.circle(annotated_image, (x, y), point_size + 1, (255, 255, 0), -1)  # ã‚·ã‚¢ãƒ³
            else:
                cv2.circle(annotated_image, (x, y), point_size, (0, 255, 0), -1)  # ç·‘
        else:  # ãã®ä»–ã®è©³ç´°ãƒã‚¤ãƒ³ãƒˆ
            cv2.circle(annotated_image, (x, y), max(1, point_size // 2), (0, 255, 0), -1)  # å°ã•ã„ç·‘
    
    # ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯æ•°ã‚’ç”»åƒã«è¡¨ç¤º
    cv2.putText(annotated_image, f"Points: {len(landmarks)}", 
                (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
    
    return annotated_image

def main():
    st.set_page_config(page_title="é¡”å½¢çŠ¶é¡ä¼¼åº¦åˆ†æã‚¢ãƒ—ãƒª", layout="wide")
    st.title("é¡”å½¢çŠ¶é¡ä¼¼åº¦åˆ†æã‚¢ãƒ—ãƒª")
    
    mode = st.sidebar.selectbox("ãƒ¢ãƒ¼ãƒ‰é¸æŠ", ["è‡ªå‹•è§£æãƒ¢ãƒ¼ãƒ‰", "æ‰‹å‹•æ³¨é‡ˆãƒ¢ãƒ¼ãƒ‰"])
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.subheader("åŸºæº–ç”»åƒ")
        uploaded_base = st.file_uploader("åŸºæº–ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=['jpg', 'jpeg', 'png'], key="base")
    
    with col2:
        st.subheader("æ¯”è¼ƒç”»åƒ1")
        uploaded_comp1 = st.file_uploader("æ¯”è¼ƒç”»åƒ1ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=['jpg', 'jpeg', 'png'], key="comp1")
    
    with col3:
        st.subheader("æ¯”è¼ƒç”»åƒ2")
        input_method = st.radio("å…¥åŠ›æ–¹æ³•ã‚’é¸æŠ", ["ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", "ã‚«ãƒ¡ãƒ©ã‚­ãƒ£ãƒ—ãƒãƒ£"], key="input_method")
        
        uploaded_comp2 = None
        camera_image = None
        
        if input_method == "ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰":
            uploaded_comp2 = st.file_uploader("æ¯”è¼ƒç”»åƒ2ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=['jpg', 'jpeg', 'png'], key="comp2")
        else:
            st.write("ğŸ“· ã‚«ãƒ¡ãƒ©ã§ç”»åƒã‚’ã‚­ãƒ£ãƒ—ãƒãƒ£")
            st.info("ğŸ’¡ **ã‚«ãƒ¡ãƒ©æ’®å½±ã®ã‚³ãƒ„:**\n- æ˜ã‚‹ã„å ´æ‰€ã§æ’®å½±ã—ã¦ãã ã•ã„\n- é¡”ãŒæ­£é¢ã‚’å‘ãã‚ˆã†ã«ã—ã¦ãã ã•ã„\n- ã‚«ãƒ¡ãƒ©ã‹ã‚‰é©åº¦ãªè·é›¢ã‚’ä¿ã£ã¦ãã ã•ã„")
            camera_image = st.camera_input("å†™çœŸã‚’æ’®å½±ã—ã¦ãã ã•ã„", key="camera")
            
            if camera_image is not None:
                # ã‚«ãƒ¡ãƒ©ç”»åƒã‚’PIL Imageã«å¤‰æ›
                uploaded_comp2 = camera_image
                st.success("âœ… ã‚«ãƒ¡ãƒ©ç”»åƒãŒæ’®å½±ã•ã‚Œã¾ã—ãŸï¼")
                
                # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼è¡¨ç¤º
                preview_image = Image.open(camera_image)
                st.image(preview_image, caption="æ’®å½±ã—ãŸç”»åƒã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼", width=200)
    
    if mode == "è‡ªå‹•è§£æãƒ¢ãƒ¼ãƒ‰":
        auto_analysis_mode(uploaded_base, uploaded_comp1, uploaded_comp2, col1, col2, col3)
    else:
        manual_annotation_mode(uploaded_base, uploaded_comp1, uploaded_comp2, col1, col2, col3)

def auto_analysis_mode(uploaded_base, uploaded_comp1, uploaded_comp2, col1, col2, col3):
    if uploaded_base and uploaded_comp1 and uploaded_comp2:
        landmarker = initialize_face_landmarker()
        
        st.info("ç”»åƒã‚’å‡¦ç†ä¸­...")
        
        base_image = np.array(Image.open(uploaded_base).convert('RGB'))
        comp1_image = np.array(Image.open(uploaded_comp1).convert('RGB'))
        comp2_image = np.array(Image.open(uploaded_comp2).convert('RGB'))
        
        st.write("### ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯æŠ½å‡ºçµæœ")
        
        st.write("**åŸºæº–ç”»åƒã®å‡¦ç†:**")
        base_landmarks, base_error = extract_landmarks(base_image, landmarker)
        
        st.write("**æ¯”è¼ƒç”»åƒ1ã®å‡¦ç†:**")
        comp1_landmarks, comp1_error = extract_landmarks(comp1_image, landmarker)
        
        st.write("**æ¯”è¼ƒç”»åƒ2ã®å‡¦ç†:**")
        comp2_landmarks, comp2_error = extract_landmarks(comp2_image, landmarker)
        
        # ã‚¨ãƒ©ãƒ¼è¡¨ç¤º
        errors = []
        if base_error:
            errors.append(f"åŸºæº–ç”»åƒ: {base_error}")
        if comp1_error:
            errors.append(f"æ¯”è¼ƒç”»åƒ1: {comp1_error}")
        if comp2_error:
            errors.append(f"æ¯”è¼ƒç”»åƒ2: {comp2_error}")
        
        if errors:
            st.error("ä»¥ä¸‹ã®ç”»åƒã§å•é¡ŒãŒç™ºç”Ÿã—ã¾ã—ãŸ:")
            for error in errors:
                st.write(f"- {error}")
        
        if base_landmarks is not None and comp1_landmarks is not None and comp2_landmarks is not None:
            # ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ä»˜ãç”»åƒã‚’ç”Ÿæˆ
            base_annotated = draw_landmarks_on_image(base_image, base_landmarks)
            comp1_annotated = draw_landmarks_on_image(comp1_image, comp1_landmarks)
            comp2_annotated = draw_landmarks_on_image(comp2_image, comp2_landmarks)
            
            # å…ƒç”»åƒã¨ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç”»åƒã‚’è¡¨ç¤º
            st.subheader("ğŸ” é¡”ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯æ¤œå‡ºçµæœ")
            
            with col1:
                st.write("**åŸºæº–ç”»åƒ**")
                st.image(base_image, caption="å…ƒç”»åƒ", use_container_width=True)
                st.image(base_annotated, caption="ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯æ¤œå‡ºçµæœ", use_container_width=True)
            
            with col2:
                st.write("**æ¯”è¼ƒç”»åƒ1**")
                st.image(comp1_image, caption="å…ƒç”»åƒ", use_container_width=True)
                st.image(comp1_annotated, caption="ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯æ¤œå‡ºçµæœ", use_container_width=True)
            
            with col3:
                st.write("**æ¯”è¼ƒç”»åƒ2**")
                st.image(comp2_image, caption="å…ƒç”»åƒ", use_container_width=True)
                st.image(comp2_annotated, caption="ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯æ¤œå‡ºçµæœ", use_container_width=True)
            
            # é¡ä¼¼åº¦è¨ˆç®—
            similarity1 = calculate_procrustes_similarity(base_landmarks, comp1_landmarks)
            similarity2 = calculate_procrustes_similarity(base_landmarks, comp2_landmarks)
            
            # çµæœè¡¨ç¤ºã‚»ã‚¯ã‚·ãƒ§ãƒ³
            st.markdown("---")
            st.subheader("ğŸ“Š é¡ä¼¼åº¦åˆ†æçµæœ")
            
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¡¨ç¤º
            col_metric1, col_metric2, col_metric3 = st.columns(3)
            
            with col_metric1:
                st.metric(
                    label="åŸºæº– vs æ¯”è¼ƒ1", 
                    value=f"{similarity1:.4f}",
                    delta=None,
                    help="ãƒ—ãƒ­ã‚¯ãƒ©ã‚¹ãƒ†ã‚¹ä¸ä¸€è‡´åº¦ï¼ˆå€¤ãŒå°ã•ã„ã»ã©é¡ä¼¼ï¼‰"
                )
            
            with col_metric2:
                st.metric(
                    label="åŸºæº– vs æ¯”è¼ƒ2", 
                    value=f"{similarity2:.4f}",
                    delta=None,
                    help="ãƒ—ãƒ­ã‚¯ãƒ©ã‚¹ãƒ†ã‚¹ä¸ä¸€è‡´åº¦ï¼ˆå€¤ãŒå°ã•ã„ã»ã©é¡ä¼¼ï¼‰"
                )
            
            with col_metric3:
                difference = abs(similarity1 - similarity2)
                st.metric(
                    label="é¡ä¼¼åº¦ã®å·®", 
                    value=f"{difference:.4f}",
                    delta=None,
                    help="2ã¤ã®é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢ã®å·®"
                )
            
            # 4æšä¸¦åˆ—æ¯”è¼ƒè¡¨ç¤º
            st.subheader("ğŸ” è©³ç´°æ¯”è¼ƒ")
            
            # ã‚ˆã‚Šæ˜ç¢ºãªçµæœè¡¨ç¤º
            if similarity1 < similarity2:
                winner = "æ¯”è¼ƒç”»åƒ1"
                winner_score = similarity1
                loser = "æ¯”è¼ƒç”»åƒ2"
                loser_score = similarity2
                st.success(f"ğŸ† **{winner}** ã®æ–¹ãŒåŸºæº–ç”»åƒã«ã‚ˆã‚Šé¡ä¼¼ã—ã¦ã„ã¾ã™ï¼ˆã‚¹ã‚³ã‚¢å·®: {difference:.4f}ï¼‰")
            else:
                winner = "æ¯”è¼ƒç”»åƒ2"
                winner_score = similarity2
                loser = "æ¯”è¼ƒç”»åƒ1"
                loser_score = similarity1
                st.success(f"ğŸ† **{winner}** ã®æ–¹ãŒåŸºæº–ç”»åƒã«ã‚ˆã‚Šé¡ä¼¼ã—ã¦ã„ã¾ã™ï¼ˆã‚¹ã‚³ã‚¢å·®: {difference:.4f}ï¼‰")
            
            # 4æšç”»åƒã®ä¸¦åˆ—è¡¨ç¤º
            col_comp1, col_comp2, col_comp3, col_comp4 = st.columns(4)
            
            with col_comp1:
                st.write("**åŸºæº–ç”»åƒ**")
                st.image(base_annotated, caption="åŸºæº–", use_container_width=True)
            
            with col_comp2:
                st.write("**æ¯”è¼ƒç”»åƒ1**")
                border_color = "green" if winner == "æ¯”è¼ƒç”»åƒ1" else "red"
                st.image(comp1_annotated, caption=f"é¡ä¼¼åº¦: {similarity1:.4f}", use_container_width=True)
                if winner == "æ¯”è¼ƒç”»åƒ1":
                    st.success("âœ… ã‚ˆã‚Šé¡ä¼¼")
                else:
                    st.info("ğŸ“Š é¡ä¼¼åº¦ä½")
            
            with col_comp3:
                st.write("**æ¯”è¼ƒç”»åƒ2**")
                st.image(comp2_annotated, caption=f"é¡ä¼¼åº¦: {similarity2:.4f}", use_container_width=True)
                if winner == "æ¯”è¼ƒç”»åƒ2":
                    st.success("âœ… ã‚ˆã‚Šé¡ä¼¼")
                else:
                    st.info("ğŸ“Š é¡ä¼¼åº¦ä½")
            
            with col_comp4:
                st.write("**çµæœã‚µãƒãƒªãƒ¼**")
                st.write("**ğŸ† å‹è€…:**")
                st.write(f"{winner}")
                st.write(f"ã‚¹ã‚³ã‚¢: {winner_score:.4f}")
                st.write("")
                st.write("**ğŸ“ˆ è©³ç´°:**")
                st.write(f"æ¤œå‡ºç‚¹æ•°: {len(base_landmarks)}ç‚¹")
                st.write(f"å‡¦ç†æ™‚é–“: æ­£å¸¸å®Œäº†")
                
                # ãƒ—ãƒ­ã‚¯ãƒ©ã‚¹ãƒ†ã‚¹è§£æã®èª¬æ˜
                with st.expander("ğŸ“š ãƒ—ãƒ­ã‚¯ãƒ©ã‚¹ãƒ†ã‚¹è§£æã¨ã¯"):
                    st.write("""
                    **ãƒ—ãƒ­ã‚¯ãƒ©ã‚¹ãƒ†ã‚¹è§£æ**ã¯2ã¤ã®å½¢çŠ¶ã®é¡ä¼¼åº¦ã‚’æ¸¬å®šã™ã‚‹çµ±è¨ˆæ‰‹æ³•ã§ã™ã€‚
                    
                    ğŸ“Œ **ç‰¹å¾´:**
                    - ä½ç½®ã€å›è»¢ã€ã‚¹ã‚±ãƒ¼ãƒ«ã®é•ã„ã‚’å–ã‚Šé™¤ã„ã¦å½¢çŠ¶ã‚’æ¯”è¼ƒ
                    - å€¤ãŒå°ã•ã„ã»ã©å½¢çŠ¶ãŒé¡ä¼¼ã—ã¦ã„ã‚‹
                    - é¡”ã®ç‰¹å¾´ç‚¹ã®é…ç½®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å®šé‡çš„ã«æ¯”è¼ƒ
                    
                    ğŸ“Š **ã‚¹ã‚³ã‚¢ã®è§£é‡ˆ:**
                    - 0.00-0.05: éå¸¸ã«é¡ä¼¼
                    - 0.05-0.15: é¡ä¼¼
                    - 0.15-0.30: ã‚„ã‚„é¡ä¼¼
                    - 0.30ä»¥ä¸Š: é¡ä¼¼åº¦ä½
                    """)
        
        else:
            st.warning("ç”»åƒå‡¦ç†ã‚’å®Œäº†ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ä»¥ä¸‹ã‚’ç¢ºèªã—ã¦ãã ã•ã„:")
            st.write("- ç”»åƒã«é¡”ãŒæ˜ç¢ºã«å†™ã£ã¦ã„ã‚‹ã‹")
            st.write("- ç”»åƒã®è§£åƒåº¦ãŒååˆ†ã‹")
            st.write("- é¡”ãŒæ­£é¢ã‚’å‘ã„ã¦ã„ã‚‹ã‹")
            st.write("- ç…§æ˜æ¡ä»¶ãŒè‰¯å¥½ã‹")

def manual_annotation_mode(uploaded_base, uploaded_comp1, uploaded_comp2, col1, col2, col3):
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
    if 'manual_points' not in st.session_state:
        st.session_state.manual_points = {'base': [], 'comp1': [], 'comp2': []}
    if 'current_point_index' not in st.session_state:
        st.session_state.current_point_index = 0
    if 'current_image_step' not in st.session_state:
        st.session_state.current_image_step = 0  # 0: base, 1: comp1, 2: comp2
    
    if uploaded_base and uploaded_comp1 and uploaded_comp2:
        images = {
            'base': np.array(Image.open(uploaded_base).convert('RGB')),
            'comp1': np.array(Image.open(uploaded_comp1).convert('RGB')),
            'comp2': np.array(Image.open(uploaded_comp2).convert('RGB'))
        }
        
        image_names = ['åŸºæº–ç”»åƒ', 'æ¯”è¼ƒç”»åƒ1', 'æ¯”è¼ƒç”»åƒ2']
        image_keys = ['base', 'comp1', 'comp2']
        
        # ç¾åœ¨ã®ãƒã‚¤ãƒ³ãƒˆæ•°ã‚’ç¢ºèª
        total_points = len(st.session_state.manual_points['base'])
        
        st.subheader("ğŸ–±ï¸ æ‰‹å‹•ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³")
        st.info(f"ğŸ“ **ç¾åœ¨ã®ã‚¹ãƒ†ãƒƒãƒ—**: ãƒã‚¤ãƒ³ãƒˆ {total_points + 1} ã‚’é…ç½®ä¸­")
        
        # 3ã¤ã®ç”»åƒã‚’æ¨ªä¸¦ã³ã§è¡¨ç¤º
        col1_img, col2_img, col3_img = st.columns(3)
        
        current_step = st.session_state.current_image_step
        current_name = image_names[current_step]
        
        # ã©ã®ç”»åƒã«ã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã‹ã‚’æ˜ç¤º
        st.write(f"ğŸ‘† **æ¬¡ã«ã‚¯ãƒªãƒƒã‚¯**: {current_name}")
        
        # 3ã¤ã®ç”»åƒã‚’è¡¨ç¤ºï¼ˆç¾åœ¨ã®å¯¾è±¡ç”»åƒã‚’ãƒã‚¤ãƒ©ã‚¤ãƒˆï¼‰
        for i, (key, name) in enumerate(zip(image_keys, image_names)):
            with [col1_img, col2_img, col3_img][i]:
                plotted_image = draw_manual_points(images[key], st.session_state.manual_points[key])
                
                if i == current_step:
                    st.success(f"âœ… {name}ï¼ˆã‚¯ãƒªãƒƒã‚¯å¯¾è±¡ï¼‰")
                    # ã‚¯ãƒªãƒƒã‚¯å¯èƒ½ãªç”»åƒ - å›ºå®šã‚µã‚¤ã‚ºã§è¡¨ç¤º
                    display_width = 400
                    image_height, image_width = plotted_image.shape[:2]
                    
                    # ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã‚’ç¶­æŒã—ã¦é«˜ã•ã‚’è¨ˆç®—
                    aspect_ratio = image_height / image_width
                    display_height = int(display_width * aspect_ratio)
                    
                    coords = streamlit_image_coordinates(
                        plotted_image,
                        key=f"coords_{key}_{st.session_state.current_point_index}",
                        width=display_width,
                        height=display_height
                    )
                    
                    if coords is not None:
                        # åº§æ¨™ã‚’ã‚ªãƒªã‚¸ãƒŠãƒ«ç”»åƒã‚µã‚¤ã‚ºã«ã‚¹ã‚±ãƒ¼ãƒ«å¤‰æ›
                        scale_x = image_width / display_width
                        scale_y = image_height / display_height
                        
                        original_x = coords["x"] * scale_x
                        original_y = coords["y"] * scale_y
                        
                        st.session_state.manual_points[key].append([original_x, original_y])
                        
                        # æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã«é€²ã‚€
                        if current_step < 2:
                            st.session_state.current_image_step += 1
                        else:
                            # 3ã¤ã®ç”»åƒã™ã¹ã¦ã«ç‚¹ã‚’é…ç½®ã—ãŸã‚‰ã€æ¬¡ã®ãƒã‚¤ãƒ³ãƒˆã¸
                            st.session_state.current_image_step = 0
                            st.session_state.current_point_index += 1
                        
                        st.rerun()
                else:
                    st.info(f"ğŸ“‹ {name}")
                    # è¡¨ç¤ºã®ã¿ã®ç”»åƒ
                    st.image(plotted_image, use_container_width=True)
                
                # ç¾åœ¨ã®ç‚¹æ•°ã‚’è¡¨ç¤º
                point_count = len(st.session_state.manual_points[key])
                st.write(f"é…ç½®æ¸ˆã¿: {point_count}ç‚¹")
        
        # ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ãƒœã‚¿ãƒ³
        st.markdown("---")
        col_btn1, col_btn2, col_btn3, col_btn4 = st.columns(4)
        
        with col_btn1:
            if st.button("ğŸ”™ å‰ã®ç‚¹ã«æˆ»ã‚‹"):
                if st.session_state.current_image_step > 0:
                    st.session_state.current_image_step -= 1
                elif total_points > 0:
                    # å‰ã®ãƒã‚¤ãƒ³ãƒˆã‚»ãƒƒãƒˆã«æˆ»ã‚‹
                    for key in image_keys:
                        if st.session_state.manual_points[key]:
                            st.session_state.manual_points[key].pop()
                    st.session_state.current_image_step = 2
                    if st.session_state.current_point_index > 0:
                        st.session_state.current_point_index -= 1
                st.rerun()
        
        with col_btn2:
            if st.button("ğŸ—‘ï¸ å…¨ã¦å‰Šé™¤"):
                st.session_state.manual_points = {'base': [], 'comp1': [], 'comp2': []}
                st.session_state.current_point_index = 0
                st.session_state.current_image_step = 0
                st.rerun()
        
        with col_btn3:
            if st.button("â­ï¸ ã‚¹ã‚­ãƒƒãƒ—"):
                # ç¾åœ¨ã®ç”»åƒã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦æ¬¡ã¸
                if current_step < 2:
                    st.session_state.current_image_step += 1
                else:
                    st.session_state.current_image_step = 0
                    st.session_state.current_point_index += 1
                st.rerun()
        
        # é€²æ—è¡¨ç¤º
        points_counts = [len(st.session_state.manual_points[k]) for k in image_keys]
        min_points = min(points_counts)
        
        st.write("ğŸ“Š **é€²æ—çŠ¶æ³**:")
        progress_col1, progress_col2, progress_col3 = st.columns(3)
        
        for i, (count, name) in enumerate(zip(points_counts, image_names)):
            with [progress_col1, progress_col2, progress_col3][i]:
                st.metric(name, f"{count}ç‚¹")
        
        # é¡ä¼¼åº¦è¨ˆç®—
        if min_points >= 3 and len(set(points_counts)) == 1:
            with col_btn4:
                if st.button("ğŸ§® é¡ä¼¼åº¦è¨ˆç®—"):
                    base_points = np.array(st.session_state.manual_points['base'])
                    comp1_points = np.array(st.session_state.manual_points['comp1'])
                    comp2_points = np.array(st.session_state.manual_points['comp2'])
                    
                    similarity1 = calculate_procrustes_similarity(base_points, comp1_points)
                    similarity2 = calculate_procrustes_similarity(base_points, comp2_points)
                    
                    st.markdown("---")
                    st.subheader("ğŸ“Š æ‰‹å‹•æ³¨é‡ˆã«ã‚ˆã‚‹é¡ä¼¼åº¦åˆ†æçµæœ")
                    
                    col_result1, col_result2 = st.columns(2)
                    
                    with col_result1:
                        st.metric("åŸºæº– vs æ¯”è¼ƒ1", f"{similarity1:.4f}", help="å€¤ãŒå°ã•ã„ã»ã©é¡ä¼¼")
                    
                    with col_result2:
                        st.metric("åŸºæº– vs æ¯”è¼ƒ2", f"{similarity2:.4f}", help="å€¤ãŒå°ã•ã„ã»ã©é¡ä¼¼")
                    
                    if similarity1 < similarity2:
                        st.success("ğŸ† æ¯”è¼ƒç”»åƒ1ã®æ–¹ãŒåŸºæº–ç”»åƒã«ã‚ˆã‚Šé¡ä¼¼ã—ã¦ã„ã¾ã™")
                    else:
                        st.success("ğŸ† æ¯”è¼ƒç”»åƒ2ã®æ–¹ãŒåŸºæº–ç”»åƒã«ã‚ˆã‚Šé¡ä¼¼ã—ã¦ã„ã¾ã™")
                    
                    # ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³çµæœã®4æšè¡¨ç¤º
                    st.subheader("ğŸ” ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³çµæœæ¯”è¼ƒ")
                    result_col1, result_col2, result_col3, result_col4 = st.columns(4)
                    
                    with result_col1:
                        st.write("**åŸºæº–ç”»åƒ**")
                        base_annotated = draw_manual_points(images['base'], st.session_state.manual_points['base'])
                        st.image(base_annotated, use_container_width=True)
                    
                    with result_col2:
                        st.write("**æ¯”è¼ƒç”»åƒ1**")
                        comp1_annotated = draw_manual_points(images['comp1'], st.session_state.manual_points['comp1'])
                        st.image(comp1_annotated, use_container_width=True)
                        st.metric("é¡ä¼¼åº¦", f"{similarity1:.4f}")
                    
                    with result_col3:
                        st.write("**æ¯”è¼ƒç”»åƒ2**")
                        comp2_annotated = draw_manual_points(images['comp2'], st.session_state.manual_points['comp2'])
                        st.image(comp2_annotated, use_container_width=True)
                        st.metric("é¡ä¼¼åº¦", f"{similarity2:.4f}")
                    
                    with result_col4:
                        st.write("**æ³¨é‡ˆçµ±è¨ˆ**")
                        st.write(f"ç·ãƒã‚¤ãƒ³ãƒˆæ•°: {min_points}")
                        st.write(f"åŸºæº–ç”»åƒ: {len(st.session_state.manual_points['base'])}ç‚¹")
                        st.write(f"æ¯”è¼ƒç”»åƒ1: {len(st.session_state.manual_points['comp1'])}ç‚¹")
                        st.write(f"æ¯”è¼ƒç”»åƒ2: {len(st.session_state.manual_points['comp2'])}ç‚¹")
        
        elif min_points < 3:
            st.info(f"ğŸ’¡ **ãƒ’ãƒ³ãƒˆ**: å„ç”»åƒã«æœ€ä½3ç‚¹ãšã¤é…ç½®ã—ã¦ãã ã•ã„ã€‚ç¾åœ¨: {min_points}ç‚¹")
        elif len(set(points_counts)) != 1:
            st.warning(f"âš ï¸ å…¨ã¦ã®ç”»åƒã«åŒã˜æ•°ã®ç‚¹ã‚’é…ç½®ã—ã¦ãã ã•ã„ã€‚ç¾åœ¨: åŸºæº–{points_counts[0]}ç‚¹, æ¯”è¼ƒ1{points_counts[1]}ç‚¹, æ¯”è¼ƒ2{points_counts[2]}ç‚¹")
    
    else:
        st.info("ğŸ“· 3ã¤ã®ç”»åƒã‚’ã™ã¹ã¦ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã‹ã‚‰æ‰‹å‹•ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é–‹å§‹ã—ã¦ãã ã•ã„ã€‚")

def draw_manual_points(image, points):
    if not points:
        return image
    
    plotted_image = image.copy()
    height, width = image.shape[:2]
    point_size = max(3, min(width, height) // 150)
    
    for i, point in enumerate(points):
        x, y = int(point[0]), int(point[1])
        
        # ãƒã‚¤ãƒ³ãƒˆç•ªå·ã«ã‚ˆã£ã¦è‰²ã‚’å¤‰ãˆã‚‹
        colors = [
            (255, 0, 0),    # èµ¤
            (0, 255, 0),    # ç·‘
            (0, 0, 255),    # é’
            (255, 255, 0),  # ã‚·ã‚¢ãƒ³
            (255, 0, 255),  # ãƒã‚¼ãƒ³ã‚¿
            (0, 255, 255),  # é»„è‰²
            (128, 0, 128),  # ç´«
            (255, 165, 0),  # ã‚ªãƒ¬ãƒ³ã‚¸
        ]
        
        color = colors[i % len(colors)]
        
        # ãƒã‚¤ãƒ³ãƒˆã‚’æç”»
        cv2.circle(plotted_image, (x, y), point_size + 2, (255, 255, 255), -1)  # ç™½ã„èƒŒæ™¯
        cv2.circle(plotted_image, (x, y), point_size, color, -1)  # ã‚«ãƒ©ãƒ¼ãƒã‚¤ãƒ³ãƒˆ
        
        # ãƒã‚¤ãƒ³ãƒˆç•ªå·ã‚’æç”»
        font_scale = max(0.5, min(width, height) / 1000)
        text_x = x + point_size + 5
        text_y = y + point_size
        
        cv2.putText(plotted_image, str(i + 1), (text_x, text_y), 
                   cv2.FONT_HERSHEY_SIMPLEX, font_scale, (255, 255, 255), 2)
        cv2.putText(plotted_image, str(i + 1), (text_x, text_y), 
                   cv2.FONT_HERSHEY_SIMPLEX, font_scale, color, 1)
    
    # ç·ãƒã‚¤ãƒ³ãƒˆæ•°ã‚’è¡¨ç¤º
    if points:
        text = f"Points: {len(points)}"
        cv2.putText(plotted_image, text, (10, 30), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
        cv2.putText(plotted_image, text, (10, 30), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 1)
    
    return plotted_image

if __name__ == "__main__":
    main()